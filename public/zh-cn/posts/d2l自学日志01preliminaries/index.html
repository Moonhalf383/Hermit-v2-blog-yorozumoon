<!DOCTYPE html>
<html lang="zh-cn">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="UTF-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1, maximum-scale=1, minimal-ui">
<meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="robots" content="index, follow"><meta name="author" content="Moonhalf">
<meta name="description" content=""><link rel="manifest" href="/site.webmanifest"><link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<meta name="apple-mobile-web-app-title" content="Yorozumoon" />
<meta name="theme-color" content="#494f5c">
<meta name="msapplication-TileColor" content="#494f5c">

  <meta itemprop="name" content="D2L自学日志01：preliminaries">
  <meta itemprop="description" content="机器学习基础知识 多维数组 pandas（数据处理） linear-algebra（线性代数） calculus（微积分） autograd（自动微分） probability（概率） 多维数组基本知识 import torch x = torch.arange(12) # tensor([0,1,2,3,4,5,6,7,8,9,10,11]) torch.arange(x,y,z)的语法和python原生的range()语法一致，左闭右开，第三个参数为步长。">
  <meta itemprop="datePublished" content="2025-12-30T18:14:36+08:00">
  <meta itemprop="dateModified" content="2025-12-30T18:14:36+08:00">
  <meta itemprop="wordCount" content="5785">
  <meta itemprop="keywords" content="D2L,Python,Torch,机器学习"><meta property="og:url" content="http://localhost:1313/zh-cn/posts/d2l%E8%87%AA%E5%AD%A6%E6%97%A5%E5%BF%9701preliminaries/">
  <meta property="og:site_name" content="Yorozumoon">
  <meta property="og:title" content="D2L自学日志01：preliminaries">
  <meta property="og:description" content="机器学习基础知识 多维数组 pandas（数据处理） linear-algebra（线性代数） calculus（微积分） autograd（自动微分） probability（概率） 多维数组基本知识 import torch x = torch.arange(12) # tensor([0,1,2,3,4,5,6,7,8,9,10,11]) torch.arange(x,y,z)的语法和python原生的range()语法一致，左闭右开，第三个参数为步长。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-30T18:14:36+08:00">
    <meta property="article:modified_time" content="2025-12-30T18:14:36+08:00">
    <meta property="article:tag" content="D2L">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="Torch">
    <meta property="article:tag" content="机器学习">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="D2L自学日志01：preliminaries">
  <meta name="twitter:description" content="机器学习基础知识 多维数组 pandas（数据处理） linear-algebra（线性代数） calculus（微积分） autograd（自动微分） probability（概率） 多维数组基本知识 import torch x = torch.arange(12) # tensor([0,1,2,3,4,5,6,7,8,9,10,11]) torch.arange(x,y,z)的语法和python原生的range()语法一致，左闭右开，第三个参数为步长。">

<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "D2L自学日志01：preliminaries",
    "name": "D2L自学日志01：preliminaries",
    "description": "机器学习基础知识 多维数组 pandas（数据处理） linear-algebra（线性代数） calculus（微积分） autograd（自动微分） probability（概率） 多维数组基本知识 import torch x = torch.arange(12) # tensor([0,1,2,3,4,5,6,7,8,9,10,11]) torch.arange(x,y,z)的语法和python原生的range()语法一致，左闭右开，第三个参数为步长。\n",
    "keywords": ["D2L", "python", "torch", "机器学习"],
    "articleBody": "机器学习基础知识 多维数组 pandas（数据处理） linear-algebra（线性代数） calculus（微积分） autograd（自动微分） probability（概率） 多维数组基本知识 import torch x = torch.arange(12) # tensor([0,1,2,3,4,5,6,7,8,9,10,11]) torch.arange(x,y,z)的语法和python原生的range()语法一致，左闭右开，第三个参数为步长。\nx.shape 一个张量的形状，返回值为一个一个torch.Size对象。你可以把它看成一个列表，直接通过方括号访问维度索引值可以得到该张量的某个维度的大小。\n如：\nimport torch x = torch.arange(24).reshape([2,3,4]) print(x.shape) # torch.Size([2,3,4]) print(x.shape[1]) # 3 torch.reshape([x,y,z,...])可以修改一个张量的形状，本质上就是先把这个张量拉成一维，再依次按照对应维度的大小对一维张量进行均分。\n比如：一个x = torch.arange(24)，我们对它进行.reshape([2,3,4])操作，你可以看做：一个arange(24)先进行shape[0]处数字均分，变成前十二个和后十二个，然后对两个12长度的子张量再按照shape[1]处数字进行均分，分别变为三个长度4的子张量，最后对每个长度4的子张量再次均分，得到长度为1的子张量。每一次均分都代表进入下一个维度。\nx.numel()返回x张量的总元素个数。\ntorch.zeros((2,3,4))生成一个形状为torch.Size([2,3,4])的所有元素为0的张量。\ntorch.ones((2,3,4))生成一个形状为torch.Size([2,3,4])的所有元素为1的张量。\n运算符 常见的标准运算符（+，-，*，\\，**）都会被升级为按元素计算。\ntorch.exp(x)得到一个形状和x一致，但是对应位置元素为exp(x)的张量。\ntorch.cat((X, Y), dim = 1)将两个张量沿1维度对接起来，前提是两者其他维度形状一致。\n如：\nX = torch.arange(12, dtype=torch.float32).reshape((3,4)) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) 结果：\n(tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]), tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.], [ 4., 5., 6., 7., 1., 2., 3., 4.], [ 8., 9., 10., 11., 4., 3., 2., 1.]])) X == Y得到一个张量，元素为布尔值，若X和Y对应位置相等，则对应位置的结果为True，否则为False。\nX.sum()得到一个单元素张量，数值为X所有元素的总和，如果想直接得到这个值，应该写X.sum().item()。\n广播机制 当尝试对两个形状并不匹配的张量相互计算时，若不匹配的维度中某个张量的长度为1，torch就会自动在这个维度上通过复制的方式使两个张量强行匹配数据。\n如：\na = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) a, b 结果为：\n(tensor([[0], [1], [2]]), tensor([[0, 1]])) 但是：\na + b 结果为：\ntensor([[0, 1], [1, 2], [2, 3]]) 切片 X[2, 3:5]表示的就是X张量中0维度坐标2、1维度坐标3，4的元素。如果我们想要给多个元素赋同样的值，我们只需要索引所有元素，然后赋值。\nx = torch.arange(12).reshape([3, 4]) x[0:2, :] = 12 得到的x：\ntensor([[12., 12., 12., 12.], [12., 12., 12., 12.], [ 8., 9., 10., 11.]]) 转换为其他对象 转换为numpy多维数组：\nx = x.numpy() 转换为torch.Tensor：\nx = torch.Tensor(x) pandas 数据预处理。\n写入数据：\nimport os os.makedirs(os.path.join('..', 'data'), exist_ok=True) data_file = os.path.join('..', 'data', 'house_tiny.csv') with open(data_file, 'w') as f: f.write('NumRooms,Alley,Price\\n') # 列名 f.write('NA,Pave,127500\\n') # 每行表示一个数据样本 f.write('2,NA,106000\\n') f.write('4,NA,178100\\n') f.write('NA,NA,140000\\n') 读取数据：\nimport pandas as pd data = pd.read_csv(data_file) print(data) 处理缺失值 将nan值填补为平均值：\ninputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2] inputs = inputs.fillna(inputs.mean()) print(inputs) pandas部分练习 import pandas as pd import random import os import torch def generate_row(): row_data = '' for i in range(5): added_data = random.random() if added_data \u003c= 0.2: added_data = \"NA\" row_data += str(added_data) if i != 4: row_data += ',' row_data += '\\n' return row_data os.makedirs(os.path.join('..', 'Moonhalf_data'), exist_ok = True) mh_file = os.path.join('..', 'Moonhalf_data', 'mh_data.csv') with open(mh_file, 'w') as f: f.write('NumA,NumB,NumC,NumD,NumE\\n') # 列名 for i in range(50): f.write(generate_row()) data = pd.read_csv(mh_file) print(data) max_na = data.isna().sum().max() # print(target_column) refined_data = data.loc[:, data.isna().sum() \u003c max_na] print(refined_data) refined_data = refined_data.fillna(refined_data.mean()) refined_tensor = torch.tensor(refined_data.to_numpy(dtype = float)) print(refined_tensor) 这段代码的作用是：生成一个50 * 5的随机数数据集，删除缺失值最多的列，并将预处理后的数据集转换为张量格式。\n线性代数 X.T转置。\nX.clone()一个和X一模一样的张量，但是重新分配内存。\nA.sum(axis = 1)让A沿着1维坐标轴求和，得到n-1维张量。（想象这个张量沿着1维压扁，重叠的地方就求和）\n比如：\nimport torch test_sum = torch.arange(24).reshape(2,3,4) print(test_sum) print(test_sum.sum(axis = 0)) print(test_sum.sum(axis = 1)) print(test_sum.sum(axis = 2)) print(test_sum.sum(axis = [0, 1])) 结果：\ntensor([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) tensor([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]) tensor([[12, 15, 18, 21], [48, 51, 54, 57]]) tensor([[ 6, 22, 38], [54, 70, 86]]) tensor([60, 66, 72, 78]) A.mean(axis = 1)沿着1维求平均值，规则类似sum。\n不降维求和：\nsum_A = A.sum(axis=1, keepdims=True) sum_A 点积 torch.dot(X, Y)将X和Y对应位置元素相乘求和，得到一个单元素张量。\n注意：虽然听起来很废话，但是dot只能适用于1D张量。\n向量积 torch.mv(X, Y)对两个向量求叉乘。\n矩阵乘法 torch.mm(X, Y)对两个矩阵做矩阵乘法，注意这不是对应位置元素相乘（Hadamard积）。\n范数 用来衡量一个张量的大小。\nL2范数 torch.norm()得到一个向量的长度。对于一个多维张量来说，它的含义是求所有元素平方和再开根号。\nimport torch u = torch.tensor([3.0, -4.0]) v = torch.tensor([3.0,4.0,5.0]) torch.norm(u), torch.norm(v) 结果：\n(tensor(5.), tensor(7.0711)) L1范数 torch.abs(x).sum()得到所有分量绝对值之和。\n自动求导（autograd） import torch x = torch.tensor([2.0], requires_grad=True) y = x ** 2 y.backward() print(x.grad) # 此时是 4.0 z = x * 3 z.backward() print(x.grad) # 此时是 4.0 + 3.0 = 7.0 ！ 为什么x.grad似乎并不符合通常的程序设计原则？ x.grad在通常的oop程序设计中应该表示一个对象自身的属性，但是在torch中x.grad会随着使用x的表达式backward()而发生改变，对于一般的程序设计而言，这种设计是比较反直觉的。\n对于不同的表达式，torch对于梯度的操作并非覆盖而是累加。从上面的例子中我们可以看到x.grad最后的值为7.0。至于为什么要设计成累加的模式，一方面是数学上的便利，根据微积分的链式法则，如果一个变量x同时影响了多个分支，那么总梯度就是各个分支梯度的和；另一方面这样做更能节省内存，即使你的网络极其复杂，分成好几路输出，PyTorch也不需要为每一路都开辟新的空间存梯度，只需要在x.grad这一相同地址上做加法即可。\nimport torch x = torch.arange(24).reshape([4,6]).to(torch.float32) x.requires_grad_(True) y = (x * x).sum() y.backward() x.grad 需要注意的是，尽管多维数组对多维数组的求导在数学上是有意义的，pytorch中仅支持从标量开始的反向传播。比如，在如上的例子中，y是一个单元素张量，此时可以对它进行backward()操作，但是如果y = 2 * x，那么再尝试y.backward()就会报错。\n原因同样是出于对机器学习实际计算工作的妥协，在工程上直接对多维数组之间进行求导意味着我们会需要计算两个数组元素个数之积次计算，而机器学习中我们处理的数组大小有时可以达到百万数量级。直接求导会导致极大的显存消耗。\n我们尝试通过一个例子来解释pytorch的优化逻辑：\n假设：\nx = [x1, x2, …]是一个10000维向量。 y = [y1, y2, …]也是一个10000维向量，但是它是由x计算出来的。 L = f(y)是一个标量。在机器学习的语境下它通常是一个误差函数。 你的目标是求$\\frac{\\partial L}{\\partial x}$。\n对于通常的数学书求法，我们会需要使用链式法则，计算$\\frac{\\partial L}{\\partial y}$，再计算$\\frac{\\partial y}{\\partial x}$，再将两者相乘。其中后者就是向量对向量求导，即雅可比矩阵。当x、y体积巨大时，直接计算这个雅可比矩阵的计算量会直接爆炸。\n更重要的是，虽然x和y体积巨大，但是真正进行相互计算的时候，对于它们的每个分量来说，另一个向量中和它相关的分量个数是相当有限的。尤其在机器学习的情景下，假如你计算出了雅可比矩阵，你会发现它相当的稀疏，其中绝大多数的数字都是0。因为大多数的分量之间都没有什么关系。\n但是对于一个矩阵计算来说，它必须要存储所有分量和所有分量之间的关系，这意味着即使是两者没有任何关系，矩阵也必须要占用空间来表示这种没有关系的关系。\npytorch的优化思路其实很简单：我们大可以只站在分量的视角看待问题，只去记录那些和自己有关的分量以及过程中的计算方式，而对于那些和自己无关的分量，我们根本不去记录，也自然不会产生关联。\n我们来用一个相对简单的例子解释torch的工作原理：\nimport torch x = torch.Tensor([2, 3, 4]) x.requires_grad_(True) y1_mask = x \u003e 3 y1 = x[y1_mask].squeeze(0) y2 = (x * x).sum() # print(y1, y2) L = y1 + y2 print(f\"L: {L}\") L.backward() print(x.grad) 运行得到的结果是：\nL: 33.0 tensor([4., 6., 9.]) 在这个例子中，y1是x中大于3的元素构成的单元素张量，y2是x对自己的点积组成的单元素张量，L则是y1和y2求和得到的单元素张量。\n首先，我们计算y1时，torch同时记录下了“y1是由x通过y1_mask筛选出来的”；计算y2时，torch也同时记录下了“y2是由x对自己按元素求乘积再求和得到的”。最后，计算L时，torch会记录下“L是由y1加上y2得到的”。\n当我们对L进行backward时，torch会去查询L的计算方式。此时，我们假设我们需要L增加1，torch会把这个1传递给计算得到L的y1和y2，因为L对y1和y2求偏导得到的是1，所以y1和y2被传递得到1，意味着y1和y2对于L的贡献都是1。\n接着，torch会去查询y1和y2的计算方式，y1是由x的第三个元素组成的，所以x的第三个元素对y1的贡献是1，进而对L的贡献也是1；y2由x自乘求和得到，所以x每个元素对y2的贡献是2 * x，在x = [2, 3, 4]时，x中元素对y2的贡献依次为4、6、8，进而对L的贡献也是4，6，8。\n最后，我们对x中每个元素对L的各种贡献进行求和，我们得到x中每个元素在当前数值下对于L的贡献依次为4，6，9，即为我们最后得到的tensor([4.,6.,9.,])\n需要明确的是，贡献这个词的使用实际上可能数学并不准确。我们最后得到的东西实际上是L这个标量对x的梯度，即∇L，其意义可以简单理解为L在随x各个维度改变而改变的速率。\n非标量变量的反向传播 从我们上面的推导中可以发现，我们对一个标量结果进行反向传播，最后得到的向量是x中对应元素对结果的贡献值。而理论上假如我们能直接对一个向量进行反向传播，最后得到的应该是一个雅可比矩阵。\n然而，我们需要的事实上根本不是这个矩阵，我们需要的是损失向量通过这个矩阵变换得到的结果，进而更新参数。对于我们的实际应用中，这个矩阵本身没有任何应用价值。\n用更加精辟的语言来说：\nPytorch的反向传播根本不需要返回一个矩阵，因为它本身就在模拟雅可比矩阵的线性算子功能。\n然而，在某些情景下我们确实会需要计算非标量变量的反向传播，比如多个batch同时进行训练，但是即使在这种情景，我们最后需要得到的依然是一个向量。实现方式是让批量中每个样本对x求梯度，最后再求和或者求平均，或者加权平均。总而言之，反向传播归根结底是获取一个用来更新参数的向量，获取二维及以上的张量没有什么实际意义。\n# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。 # 本例只想求偏导数的和，所以传递一个1的梯度是合适的 x.grad.zero_() print(x.grad, x) y = x * x print(y) # 等价于y.backward(torch.ones(len(x))) y.sum().backward() x.grad 所谓的gradient参数，实际上是一个和x一样长度的向量，含义是y的每个分量对于最终结果的贡献。在这里，如果我们写y.sum()，本质上就是将y的每个元素进行求和，所以y的每个元素对最终结果的贡献为1；如果写torch.ones(len(x))意思就是y的每个参数对于最终结果的贡献都是1，所以两者其实是等价的。\n分离计算 有时候我们只想要把一个torch.Tensor当做一个常数来计算，但是得到这个torch.Tensor的过程中经过了一些计算，使得最后反向传播的时候可能影响最后的结果。此时，我们会希望得到这个tensor，但是抹除它所有的计算历史。\nimport torch x = torch.arange(4) x.grad.zero_() y = x * x u = y.detach() print(u) z = u * x z.sum().backward() x.grad == u 在这个例子中，u被赋值为y.detach()，含义是使u的张量内容和y一样，但是失去了所有计算记录。这样，后续的反向传递过程中u处就不会向后传递。\n结果是：\ntensor([True, True, True, True]) 如何实现二阶导? import torch x = torch.tensor([2.0], requires_grad=True) # 1. 定义函数 y = x ** 3 # 2. 计算一阶导数 # 使用 grad 函数，并开启 create_graph=True # 这会把“计算一阶导数”的过程也记录在计算图里 grads = torch.autograd.grad(y, x, create_graph=True)[0] print(f\"一阶导数在 x=2 时: {grads.item()}\") # 3 * 2^2 = 12.0 # 3. 对一阶导数再次求导 # 这次不需要再创建图了（除非你要算三阶导） grads2 = torch.autograd.grad(grads, x)[0] print(f\"二阶导数在 x=2 时: {grads2.item()}\") # 6 * 2 = 12.0 我们没办法通过backward实现二阶导（比如L.backward().backward()），因为反向传播的计算本身并不会被存储在计算图中，但是为了实现二阶导，我们需要一阶导的计算图。因此在上面的例子中我们使用torch.autograd.grad()函数，并添加create_graph=True参数来生成包含一阶导计算图的梯度向量。\n控制流 import torch a = torch.randn(size=(12,), requires_grad=True) print(a) if a.norm() \u003e 3: b = a.sum() else: b = (a * a).sum() b.backward() print(a.grad) torch支持控制流梯度计算，这意味着即使不同情况下计算b的方式可能不同，对b进行反向传播仍然可以得到当前计算方式下的对应梯度。\n绘制函数与导函数图像（不直接计算导数） import matplotlib from matplotlib_inline import backend_inline import numpy as np from d2l import torch as d2l import math def use_svg_display(): #@save \"\"\"使用svg格式在Jupyter中显示绘图\"\"\" backend_inline.set_matplotlib_formats('svg') def set_figsize(figsize=(3.5, 2.5)): #@save \"\"\"设置matplotlib的图表大小\"\"\" use_svg_display() d2l.plt.rcParams['figure.figsize'] = figsize #@save def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend): \"\"\"设置matplotlib的轴\"\"\" axes.set_xlabel(xlabel) axes.set_ylabel(ylabel) axes.set_xscale(xscale) axes.set_yscale(yscale) axes.set_xlim(xlim) axes.set_ylim(ylim) if legend: axes.legend(legend) axes.grid() #@save def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None): \"\"\"绘制数据点\"\"\" if legend is None: legend = [] set_figsize(figsize) axes = axes if axes else d2l.plt.gca() # 如果X有一个轴，输出True def has_one_axis(X): return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list) and not hasattr(X[0], \"__len__\")) if has_one_axis(X): X = [X] if Y is None: X, Y = [[]] * len(X), X elif has_one_axis(Y): Y = [Y] if len(X) != len(Y): X = X * len(Y) axes.cla() for x, y, fmt in zip(X, Y, fmts): if len(x): axes.plot(x, y, fmt) else: axes.plot(y, fmt) set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend) x = d2l.arange(0.0, 2 * math.pi, 0.05*math.pi) x.requires_grad_(True) def f(x): return d2l.sin(x) y = f(x).sum() y.backward() plot(x.detach(), [f(x).detach(), x.grad],\"x\", \"f(x)\") 需要使用d2l库和jupyter lab。\n概率 多项分布(multinomial distribution) 将概率分配给一些离散选项的分布称为多项分布。\nimport torch from torch.distributions import multinomial fair_probs = torch.ones([6]) / 6 print(multinomial.Multinomial(1, fair_probs).sample()) print(multinomial.Multinomial(2, fair_probs).sample()) print(multinomial.Multinomial(10, fair_probs).sample()) multinomial.Multinomial()用来生成一个采样器，这个采样器每次采样会根据fair_probs也就是概率向量对应的概率来抽取一个位置索引，第一个参数时一次采样的抽取次数。最后得到的向量中对应位置的数字即这个位置被抽中的次数。\n上述代码的某次生成结果为：\ntensor([1., 0., 0., 0., 0., 0.]) tensor([0., 2., 0., 0., 0., 0.]) tensor([1., 1., 0., 2., 1., 5.]) 联合概率 \\(P(A=a,B=b)\\)，A和B同时发生的概率。\n条件概率 \\(0 \\leq \\frac{P(A=a, B=b)}{P(A=a)} \\leq 1\\) ，A发生的前提下A、B同时发生的概率。\n贝叶斯定理 $$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}.$$边际化 $$P(B) = \\sum_{A} P(A, B),$$",
    "wordCount" : "5785",
    "inLanguage": "zh-cn",
    "datePublished": "2025-12-30T18:14:36+08:00",
    "dateModified": "2025-12-30T18:14:36+08:00",
    "author":{
        "@type": "Person",
        "name": "Moonhalf",
        "url": "http://localhost:1313/zh-cn/%E6%9C%88%E3%81%AE%E3%81%BE%E3%81%AB%E3%81%BE%E3%81%AB/"
        },
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http://localhost:1313/zh-cn/posts/d2l%E8%87%AA%E5%AD%A6%E6%97%A5%E5%BF%9701preliminaries/"
    },
    "publisher": {
      "@type": "Organization",
      "name": "Yorozumoon",
      "description": "",
      "logo": {
        "@type": "ImageObject",
        "url": "http://localhost:1313/favicon.ico"
      }
    }
}
</script><title>D2L自学日志01：preliminaries</title><link rel="stylesheet dns-prefetch preconnect preload prefetch" as="style" media="screen" href="http://localhost:1313/css/style.min.7c4faf57019635cba7aa1b7c6581baeb1ec040f9c7d861be87c059d42585582c.css" integrity="sha256-fE+vVwGWNcunqht8ZYG66x7AQPnH2GG+h8BZ1CWFWCw=" crossorigin="anonymous">
	</head>

<body id="page">
<header id="site-header" class="animated slideInUp">
	<div class="hdr-wrapper section-inner">
		<div class="hdr-left">
			<div class="site-branding">
				<a href="http://localhost:1313/">Yorozumoon</a>
			</div>
			<nav class="site-nav hide-in-mobile"><a href="http://localhost:1313/zh-cn/posts/">文章</a><a href="http://localhost:1313/zh-cn/tags/">标签</a><a href="http://localhost:1313/zh-cn/about/">关于</a><a href="http://localhost:1313/zh-cn/links/">友链</a></nav>
		</div>
		<div class="hdr-right hdr-icons">
			<button id="toc-btn" class="hdr-btn desktop-only-ib" title="Table Of Contents"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-list">
      <line x1="8" y1="6" x2="21" y2="6"></line>
      <line x1="8" y1="12" x2="21" y2="12"></line>
      <line x1="8" y1="18" x2="21" y2="18"></line>
      <line x1="3" y1="6" x2="3" y2="6"></line>
      <line x1="3" y1="12" x2="3" y2="12"></line>
      <line x1="3" y1="18" x2="3" y2="18"></line>
   </svg></button><span class="hdr-links hide-in-mobile"><a href="https://github.com/Moonhalf383" target="_blank" rel="noopener me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none"
   stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
   <path
      d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
   </path>
</svg></a></span><button id="menu-btn" class="hdr-btn" title="Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
   </svg></button>
		</div>
	</div>
</header>
<div id="mobile-menu" class="animated fast">
	<ul>
		<li><a href="http://localhost:1313/zh-cn/posts/">文章</a></li>
		<li><a href="http://localhost:1313/zh-cn/tags/">标签</a></li>
		<li><a href="http://localhost:1313/zh-cn/about/">关于</a></li>
		<li><a href="http://localhost:1313/zh-cn/links/">友链</a></li>
	</ul>
</div>

	<main class="site-main section-inner animated fadeIn faster"><article class="thin">
			<header class="post-header">
				<div class="post-date"><span>Dec 30, 2025</span></div>
				<h1>D2L自学日志01：preliminaries</h1>
			</header>
			<div class="post-description"><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
   stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-feather">
   <path d="M20.24 12.24a6 6 0 0 0-8.49-8.49L5 10.5V19h8.5z"></path>
   <line x1="16" y1="8" x2="2" y2="22"></line>
   <line x1="17.5" y1="15" x2="9" y2="15"></line>
</svg><a href="http://localhost:1313/zh-cn/%E6%9C%88%E3%81%AE%E3%81%BE%E3%81%AB%E3%81%BE%E3%81%AB/" target="_blank">Moonhalf</a></p>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon">
      <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path>
      <line x1="7" y1="7" x2="7" y2="7"></line>
   </svg><span class="tag"><a href="http://localhost:1313/zh-cn/tags/d2l">D2L</a></span><span class="tag"><a href="http://localhost:1313/zh-cn/tags/python">python</a></span><span class="tag"><a href="http://localhost:1313/zh-cn/tags/torch">torch</a></span><span class="tag"><a href="http://localhost:1313/zh-cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a></span></p>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
      <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
      <polyline points="14 2 14 8 20 8"></polyline>
      <line x1="16" y1="13" x2="8" y2="13"></line>
      <line x1="16" y1="17" x2="8" y2="17"></line>
      <polyline points="10 9 9 9 8 9"></polyline>
   </svg>5785&nbspWords 阅读时长26 Minutes, 17 Seconds</p>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
      <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
      <line x1="16" y1="2" x2="16" y2="6"></line>
      <line x1="8" y1="2" x2="8" y2="6"></line>
      <line x1="3" y1="10" x2="21" y2="10"></line>
   </svg>2025-12-30 10:14 &#43;0000
</p></div>
			<hr class="post-end">
			<div class="content">
				 <h1 id="机器学习基础知识">机器学习基础知识<a href="#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h1>
<ul>
<li>多维数组</li>
<li>pandas（数据处理）</li>
<li>linear-algebra（线性代数）</li>
<li>calculus（微积分）</li>
<li>autograd（自动微分）</li>
<li>probability（概率）</li>
</ul>
<hr>
<h2 id="多维数组基本知识">多维数组基本知识<a href="#%e5%a4%9a%e7%bb%b4%e6%95%b0%e7%bb%84%e5%9f%ba%e6%9c%ac%e7%9f%a5%e8%af%86" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">12</span>) <span style="color:#75715e"># tensor([0,1,2,3,4,5,6,7,8,9,10,11])</span>
</span></span></code></pre></div><p><code>torch.arange(x,y,z)</code>的语法和python原生的<code>range()</code>语法一致，左闭右开，第三个参数为步长。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><p>一个张量的形状，返回值为一个一个<code>torch.Size</code>对象。你可以把它看成一个列表，直接通过方括号访问维度索引值可以得到该张量的某个维度的大小。</p>
<p>如：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">24</span>)<span style="color:#f92672">.</span>reshape([<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>])
</span></span><span style="display:flex;"><span>print(x<span style="color:#f92672">.</span>shape) <span style="color:#75715e"># torch.Size([2,3,4])</span>
</span></span><span style="display:flex;"><span>print(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]) <span style="color:#75715e"># 3</span>
</span></span></code></pre></div><p><code>torch.reshape([x,y,z,...])</code>可以修改一个张量的形状，本质上就是先把这个张量拉成一维，再依次按照对应维度的大小对一维张量进行均分。</p>
<p>比如：一个<code>x = torch.arange(24)</code>，我们对它进行<code>.reshape([2,3,4])</code>操作，你可以看做：一个<code>arange(24)</code>先进行shape[0]处数字均分，变成前十二个和后十二个，然后对两个12长度的子张量再按照shape[1]处数字进行均分，分别变为三个长度4的子张量，最后对每个长度4的子张量再次均分，得到长度为1的子张量。每一次均分都代表进入下一个维度。</p>
<p><code>x.numel()</code>返回x张量的总元素个数。</p>
<p><code>torch.zeros((2,3,4))</code>生成一个形状为<code>torch.Size([2,3,4])</code>的所有元素为0的张量。</p>
<p><code>torch.ones((2,3,4))</code>生成一个形状为<code>torch.Size([2,3,4])</code>的所有元素为1的张量。</p>
<hr>
<h3 id="运算符">运算符<a href="#%e8%bf%90%e7%ae%97%e7%ac%a6" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>常见的标准运算符（+，-，*，\，**）都会被升级为按元素计算。</p>
<p><code>torch.exp(x)</code>得到一个形状和x一致，但是对应位置元素为exp(x)的张量。</p>
<p><code>torch.cat((X, Y), dim = 1)</code>将两个张量沿1维度对接起来，前提是两者其他维度形状一致。</p>
<p>如：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">12</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)<span style="color:#f92672">.</span>reshape((<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>Y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([[<span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>], [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>]])
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>cat((X, Y), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>), torch<span style="color:#f92672">.</span>cat((X, Y), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>结果：</p>
<pre tabindex="0"><code>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]),
 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
</code></pre><p><code>X == Y</code>得到一个张量，元素为布尔值，若X和Y对应位置相等，则对应位置的结果为True，否则为False。</p>
<p><code>X.sum()</code>得到一个单元素张量，数值为X所有元素的总和，如果想直接得到这个值，应该写<code>X.sum().item()</code>。</p>
<hr>
<h3 id="广播机制">广播机制<a href="#%e5%b9%bf%e6%92%ad%e6%9c%ba%e5%88%b6" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>当尝试对两个形状并不匹配的张量相互计算时，若不匹配的维度中某个张量的长度为1，torch就会自动在这个维度上通过复制的方式使两个张量强行匹配数据。</p>
<p>如：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>a <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">3</span>)<span style="color:#f92672">.</span>reshape((<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>reshape((<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>a, b
</span></span></code></pre></div><p>结果为：</p>
<pre tabindex="0"><code>(tensor([[0],
         [1],
         [2]]),
 tensor([[0, 1]]))
</code></pre><p>但是：</p>
<pre tabindex="0"><code>a + b
</code></pre><p>结果为：</p>
<pre tabindex="0"><code>tensor([[0, 1],
        [1, 2],
        [2, 3]])
</code></pre><hr>
<h3 id="切片">切片<a href="#%e5%88%87%e7%89%87" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p><code>X[2, 3:5]</code>表示的就是X张量中0维度坐标2、1维度坐标3，4的元素。如果我们想要给多个元素赋同样的值，我们只需要索引所有元素，然后赋值。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">12</span>)<span style="color:#f92672">.</span>reshape([<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>])
</span></span><span style="display:flex;"><span>x[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">2</span>, :] <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>
</span></span></code></pre></div><p>得到的x：</p>
<pre tabindex="0"><code>tensor([[12., 12., 12., 12.],
        [12., 12., 12., 12.],
        [ 8.,  9., 10., 11.]])
</code></pre><hr>
<h3 id="转换为其他对象">转换为其他对象<a href="#%e8%bd%ac%e6%8d%a2%e4%b8%ba%e5%85%b6%e4%bb%96%e5%af%b9%e8%b1%a1" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>转换为numpy多维数组：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>numpy()
</span></span></code></pre></div><p>转换为torch.Tensor：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(x)
</span></span></code></pre></div><hr>
<h2 id="pandas">pandas<a href="#pandas" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<p>数据预处理。</p>
<p>写入数据：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>makedirs(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#39;..&#39;</span>, <span style="color:#e6db74">&#39;data&#39;</span>), exist_ok<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>data_file <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#39;..&#39;</span>, <span style="color:#e6db74">&#39;data&#39;</span>, <span style="color:#e6db74">&#39;house_tiny.csv&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(data_file, <span style="color:#e6db74">&#39;w&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#39;NumRooms,Alley,Price</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)  <span style="color:#75715e"># 列名</span>
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#39;NA,Pave,127500</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)  <span style="color:#75715e"># 每行表示一个数据样本</span>
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#39;2,NA,106000</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#39;4,NA,178100</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#39;NA,NA,140000</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
</span></span></code></pre></div><p>读取数据：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(data_file)
</span></span><span style="display:flex;"><span>print(data)
</span></span></code></pre></div><hr>
<h3 id="处理缺失值">处理缺失值<a href="#%e5%a4%84%e7%90%86%e7%bc%ba%e5%a4%b1%e5%80%bc" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>将nan值填补为平均值：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>inputs, outputs <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>iloc[:, <span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">2</span>], data<span style="color:#f92672">.</span>iloc[:, <span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> inputs<span style="color:#f92672">.</span>fillna(inputs<span style="color:#f92672">.</span>mean())
</span></span><span style="display:flex;"><span>print(inputs)
</span></span></code></pre></div><hr>
<h3 id="pandas部分练习">pandas部分练习<a href="#pandas%e9%83%a8%e5%88%86%e7%bb%83%e4%b9%a0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate_row</span>():
</span></span><span style="display:flex;"><span>    row_data <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>        added_data <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>random()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> added_data <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">0.2</span>:
</span></span><span style="display:flex;"><span>            added_data <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;NA&#34;</span>
</span></span><span style="display:flex;"><span>        row_data <span style="color:#f92672">+=</span> str(added_data)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">!=</span> <span style="color:#ae81ff">4</span>:
</span></span><span style="display:flex;"><span>            row_data <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#39;,&#39;</span>
</span></span><span style="display:flex;"><span>    row_data <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> row_data
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>makedirs(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#39;..&#39;</span>, <span style="color:#e6db74">&#39;Moonhalf_data&#39;</span>), exist_ok <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>mh_file <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#39;..&#39;</span>, <span style="color:#e6db74">&#39;Moonhalf_data&#39;</span>, <span style="color:#e6db74">&#39;mh_data.csv&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(mh_file, <span style="color:#e6db74">&#39;w&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#39;NumA,NumB,NumC,NumD,NumE</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)  <span style="color:#75715e"># 列名</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50</span>):
</span></span><span style="display:flex;"><span>        f<span style="color:#f92672">.</span>write(generate_row())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(mh_file)
</span></span><span style="display:flex;"><span>print(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>max_na <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>isna()<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>max()
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print(target_column)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>refined_data <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>loc[:, data<span style="color:#f92672">.</span>isna()<span style="color:#f92672">.</span>sum() <span style="color:#f92672">&lt;</span> max_na]
</span></span><span style="display:flex;"><span>print(refined_data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>refined_data <span style="color:#f92672">=</span> refined_data<span style="color:#f92672">.</span>fillna(refined_data<span style="color:#f92672">.</span>mean())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>refined_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(refined_data<span style="color:#f92672">.</span>to_numpy(dtype <span style="color:#f92672">=</span> float))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(refined_tensor)
</span></span></code></pre></div><p>这段代码的作用是：生成一个50 * 5的随机数数据集，删除缺失值最多的列，并将预处理后的数据集转换为张量格式。</p>
<hr>
<h2 id="线性代数">线性代数<a href="#%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<p><code>X.T</code>转置。</p>
<p><code>X.clone()</code>一个和X一模一样的张量，但是重新分配内存。</p>
<p><code>A.sum(axis = 1)</code>让A沿着1维坐标轴求和，得到n-1维张量。（想象这个张量沿着1维压扁，重叠的地方就求和）</p>
<p>比如：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>test_sum <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">24</span>)<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>print(test_sum)
</span></span><span style="display:flex;"><span>print(test_sum<span style="color:#f92672">.</span>sum(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>print(test_sum<span style="color:#f92672">.</span>sum(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>print(test_sum<span style="color:#f92672">.</span>sum(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>print(test_sum<span style="color:#f92672">.</span>sum(axis <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>]))
</span></span></code></pre></div><p>结果：</p>
<pre tabindex="0"><code>tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
tensor([[12, 14, 16, 18],
        [20, 22, 24, 26],
        [28, 30, 32, 34]])
tensor([[12, 15, 18, 21],
        [48, 51, 54, 57]])
tensor([[ 6, 22, 38],
        [54, 70, 86]])
tensor([60, 66, 72, 78])
</code></pre><p><code>A.mean(axis = 1)</code>沿着1维求平均值，规则类似sum。</p>
<p>不降维求和：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sum_A <span style="color:#f92672">=</span> A<span style="color:#f92672">.</span>sum(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>sum_A
</span></span></code></pre></div><hr>
<h3 id="点积">点积<a href="#%e7%82%b9%e7%a7%af" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p><code>torch.dot(X, Y)</code>将X和Y对应位置元素相乘求和，得到一个单元素张量。</p>
<p>注意：虽然听起来很废话，但是dot只能适用于1D张量。</p>
<hr>
<h3 id="向量积">向量积<a href="#%e5%90%91%e9%87%8f%e7%a7%af" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p><code>torch.mv(X, Y)</code>对两个向量求叉乘。</p>
<hr>
<h3 id="矩阵乘法">矩阵乘法<a href="#%e7%9f%a9%e9%98%b5%e4%b9%98%e6%b3%95" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p><code>torch.mm(X, Y)</code>对两个矩阵做矩阵乘法，注意这不是对应位置元素相乘（Hadamard积）。</p>
<hr>
<h3 id="范数">范数<a href="#%e8%8c%83%e6%95%b0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>用来衡量一个张量的大小。</p>
<hr>
<h3 id="l2范数">L2范数<a href="#l2%e8%8c%83%e6%95%b0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p><code>torch.norm()</code>得到一个向量的长度。对于一个多维张量来说，它的含义是求所有元素平方和再开根号。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>u <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">3.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">4.0</span>])
</span></span><span style="display:flex;"><span>v <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">3.0</span>,<span style="color:#ae81ff">4.0</span>,<span style="color:#ae81ff">5.0</span>])
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>norm(u), torch<span style="color:#f92672">.</span>norm(v)
</span></span></code></pre></div><p>结果：</p>
<pre tabindex="0"><code>(tensor(5.), tensor(7.0711))
</code></pre><hr>
<h3 id="l1范数">L1范数<a href="#l1%e8%8c%83%e6%95%b0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p><code>torch.abs(x).sum()</code>得到所有分量绝对值之和。</p>
<hr>
<h2 id="自动求导autograd">自动求导（autograd）<a href="#%e8%87%aa%e5%8a%a8%e6%b1%82%e5%af%bcautograd" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">2.0</span>], requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> x <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>y<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>print(x<span style="color:#f92672">.</span>grad) <span style="color:#75715e"># 此时是 4.0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>z <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>z<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>print(x<span style="color:#f92672">.</span>grad) <span style="color:#75715e"># 此时是 4.0 + 3.0 = 7.0 ！</span>
</span></span></code></pre></div><hr>
<h3 id="为什么xgrad似乎并不符合通常的程序设计原则">为什么x.grad似乎并不符合通常的程序设计原则？<a href="#%e4%b8%ba%e4%bb%80%e4%b9%88xgrad%e4%bc%bc%e4%b9%8e%e5%b9%b6%e4%b8%8d%e7%ac%a6%e5%90%88%e9%80%9a%e5%b8%b8%e7%9a%84%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1%e5%8e%9f%e5%88%99" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p><code>x.grad</code>在通常的oop程序设计中应该表示一个对象自身的属性，但是在torch中<code>x.grad</code>会随着使用x的表达式backward()而发生改变，对于一般的程序设计而言，这种设计是比较反直觉的。</p>
<p>对于不同的表达式，torch对于梯度的操作并非覆盖而是累加。从上面的例子中我们可以看到x.grad最后的值为7.0。至于为什么要设计成累加的模式，一方面是数学上的便利，根据微积分的链式法则，如果一个变量x同时影响了多个分支，那么总梯度就是各个分支梯度的和；另一方面这样做更能节省内存，即使你的网络极其复杂，分成好几路输出，PyTorch也不需要为每一路都开辟新的空间存梯度，只需要在x.grad这一相同地址上做加法即可。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">24</span>)<span style="color:#f92672">.</span>reshape([<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">6</span>])<span style="color:#f92672">.</span>to(torch<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>requires_grad_(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> (x <span style="color:#f92672">*</span> x)<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span>y<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>grad
</span></span></code></pre></div><p>需要注意的是，尽管多维数组对多维数组的求导在数学上是有意义的，pytorch中仅支持从标量开始的反向传播。比如，在如上的例子中，y是一个单元素张量，此时可以对它进行backward()操作，但是如果<code>y = 2 * x</code>，那么再尝试<code>y.backward()</code>就会报错。</p>
<p>原因同样是出于对机器学习实际计算工作的妥协，在工程上直接对多维数组之间进行求导意味着我们会需要计算两个数组元素个数之积次计算，而机器学习中我们处理的数组大小有时可以达到百万数量级。直接求导会导致极大的显存消耗。</p>
<hr>
<p>我们尝试通过一个例子来解释pytorch的优化逻辑：</p>
<p>假设：</p>
<ul>
<li>x = [x1, x2, &hellip;]是一个10000维向量。</li>
<li>y = [y1, y2, &hellip;]也是一个10000维向量，但是它是由x计算出来的。</li>
<li>L = f(y)是一个<strong>标量</strong>。在机器学习的语境下它通常是一个误差函数。</li>
</ul>
<p>你的目标是求$\frac{\partial L}{\partial x}$。</p>
<p>对于通常的数学书求法，我们会需要使用链式法则，计算$\frac{\partial L}{\partial y}$，再计算$\frac{\partial y}{\partial x}$，再将两者相乘。其中后者就是向量对向量求导，即<strong>雅可比矩阵</strong>。当x、y体积巨大时，直接计算这个雅可比矩阵的计算量会直接爆炸。</p>
<p>更重要的是，虽然x和y体积巨大，但是真正进行相互计算的时候，对于它们的每个分量来说，另一个向量中和它相关的分量个数是相当有限的。尤其在机器学习的情景下，假如你计算出了雅可比矩阵，你会发现它相当的稀疏，其中绝大多数的数字都是0。因为大多数的分量之间都没有什么关系。</p>
<p>但是对于一个矩阵计算来说，它必须要存储所有分量和所有分量之间的关系，这意味着即使是两者没有任何关系，矩阵也必须要占用空间来表示这种没有关系的关系。</p>
<p>pytorch的优化思路其实很简单：我们大可以只站在分量的视角看待问题，只去记录那些和自己有关的分量以及过程中的计算方式，而对于那些和自己无关的分量，我们根本不去记录，也自然不会产生关联。</p>
<p>我们来用一个相对简单的例子解释torch的工作原理：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>requires_grad_(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y1_mask <span style="color:#f92672">=</span> x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y1 <span style="color:#f92672">=</span> x[y1_mask]<span style="color:#f92672">.</span>squeeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y2 <span style="color:#f92672">=</span> (x <span style="color:#f92672">*</span> x)<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print(y1, y2)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>L <span style="color:#f92672">=</span> y1 <span style="color:#f92672">+</span> y2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;L: </span><span style="color:#e6db74">{</span>L<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>L<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(x<span style="color:#f92672">.</span>grad)
</span></span></code></pre></div><p>运行得到的结果是：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>L: 33.0
</span></span><span style="display:flex;"><span>tensor<span style="color:#f92672">([</span>4., 6., 9.<span style="color:#f92672">])</span>
</span></span></code></pre></div><p>在这个例子中，y1是x中大于3的元素构成的单元素张量，y2是x对自己的点积组成的单元素张量，L则是y1和y2求和得到的单元素张量。</p>
<p>首先，我们计算y1时，torch同时记录下了“y1是由x通过y1_mask筛选出来的”；计算y2时，torch也同时记录下了“y2是由x对自己按元素求乘积再求和得到的”。最后，计算L时，torch会记录下“L是由y1加上y2得到的”。</p>
<p>当我们对L进行backward时，torch会去查询L的计算方式。此时，我们假设我们需要L增加1，torch会把这个1传递给计算得到L的y1和y2，因为L对y1和y2求偏导得到的是1，所以y1和y2被传递得到1，意味着y1和y2对于L的贡献都是1。</p>
<p>接着，torch会去查询y1和y2的计算方式，y1是由x的第三个元素组成的，所以x的第三个元素对y1的贡献是1，进而对L的贡献也是1；y2由x自乘求和得到，所以x每个元素对y2的贡献是2 * x，在x = [2, 3, 4]时，x中元素对y2的贡献依次为4、6、8，进而对L的贡献也是4，6，8。</p>
<p>最后，我们对x中每个元素对L的各种贡献进行求和，我们得到x中每个元素在当前数值下对于L的贡献依次为4，6，9，即为我们最后得到的<code>tensor([4.,6.,9.,])</code></p>
<p>需要明确的是，贡献这个词的使用实际上可能数学并不准确。我们最后得到的东西实际上是L这个标量对x的梯度，即∇L，其意义可以简单理解为L在随x各个维度改变而改变的速率。</p>
<hr>
<h3 id="非标量变量的反向传播">非标量变量的反向传播<a href="#%e9%9d%9e%e6%a0%87%e9%87%8f%e5%8f%98%e9%87%8f%e7%9a%84%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>从我们上面的推导中可以发现，我们对一个标量结果进行反向传播，最后得到的向量是x中对应元素对结果的贡献值。而理论上假如我们能直接对一个向量进行反向传播，最后得到的应该是一个雅可比矩阵。</p>
<p>然而，我们需要的事实上根本不是这个矩阵，我们需要的是损失向量通过这个矩阵变换得到的结果，进而更新参数。对于我们的实际应用中，这个矩阵本身没有任何应用价值。</p>
<p>用更加精辟的语言来说：</p>
<blockquote>
<p>Pytorch的反向传播根本不需要返回一个矩阵，因为它本身就在模拟雅可比矩阵的线性算子功能。</p>
</blockquote>
<p>然而，在某些情景下我们确实会需要计算非标量变量的反向传播，比如多个batch同时进行训练，但是即使在这种情景，我们最后需要得到的依然是一个向量。实现方式是让批量中每个样本对x求梯度，最后再求和或者求平均，或者加权平均。总而言之，反向传播归根结底是获取一个用来更新参数的向量，获取二维及以上的张量没有什么实际意义。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span>
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span>print(x<span style="color:#f92672">.</span>grad, x)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> x
</span></span><span style="display:flex;"><span>print(y)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 等价于y.backward(torch.ones(len(x)))</span>
</span></span><span style="display:flex;"><span>y<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>grad
</span></span></code></pre></div><p>所谓的gradient参数，实际上是一个和x一样长度的向量，含义是<strong>y的每个分量</strong>对于最终结果的贡献。在这里，如果我们写<code>y.sum()</code>，本质上就是将y的每个元素进行求和，所以y的每个元素对最终结果的贡献为1；如果写<code>torch.ones(len(x))</code>意思就是y的每个参数对于最终结果的贡献都是1，所以两者其实是等价的。</p>
<hr>
<h3 id="分离计算">分离计算<a href="#%e5%88%86%e7%a6%bb%e8%ae%a1%e7%ae%97" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>有时候我们只想要把一个<code>torch.Tensor</code>当做一个常数来计算，但是得到这个<code>torch.Tensor</code>的过程中经过了一些计算，使得最后反向传播的时候可能影响最后的结果。此时，我们会希望得到这个tensor，但是抹除它所有的计算历史。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> x
</span></span><span style="display:flex;"><span>u <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>print(u)
</span></span><span style="display:flex;"><span>z <span style="color:#f92672">=</span> u <span style="color:#f92672">*</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>z<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>grad <span style="color:#f92672">==</span> u
</span></span></code></pre></div><p>在这个例子中，u被赋值为<code>y.detach()</code>，含义是使u的张量内容和y一样，但是失去了所有计算记录。这样，后续的反向传递过程中u处就不会向后传递。</p>
<p>结果是：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>tensor<span style="color:#f92672">([</span>True, True, True, True<span style="color:#f92672">])</span>
</span></span></code></pre></div><hr>
<h3 id="如何实现二阶导">如何实现二阶导?<a href="#%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e4%ba%8c%e9%98%b6%e5%af%bc" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">2.0</span>], requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. 定义函数</span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> x <span style="color:#f92672">**</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. 计算一阶导数</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用 grad 函数，并开启 create_graph=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 这会把“计算一阶导数”的过程也记录在计算图里</span>
</span></span><span style="display:flex;"><span>grads <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>autograd<span style="color:#f92672">.</span>grad(y, x, create_graph<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;一阶导数在 x=2 时: </span><span style="color:#e6db74">{</span>grads<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>) <span style="color:#75715e"># 3 * 2^2 = 12.0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. 对一阶导数再次求导</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 这次不需要再创建图了（除非你要算三阶导）</span>
</span></span><span style="display:flex;"><span>grads2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>autograd<span style="color:#f92672">.</span>grad(grads, x)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;二阶导数在 x=2 时: </span><span style="color:#e6db74">{</span>grads2<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>) <span style="color:#75715e"># 6 * 2 = 12.0</span>
</span></span></code></pre></div><p>我们没办法通过backward实现二阶导（比如L.backward().backward()），因为反向传播的计算本身并不会被存储在计算图中，但是为了实现二阶导，我们需要一阶导的计算图。因此在上面的例子中我们使用<code>torch.autograd.grad()</code>函数，并添加<code>create_graph=True</code>参数来生成包含一阶导计算图的梯度向量。</p>
<hr>
<h3 id="控制流">控制流<a href="#%e6%8e%a7%e5%88%b6%e6%b5%81" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>a <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>,), requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>print(a)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> a<span style="color:#f92672">.</span>norm() <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>    b <span style="color:#f92672">=</span> a<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    b <span style="color:#f92672">=</span> (a <span style="color:#f92672">*</span> a)<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>b<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(a<span style="color:#f92672">.</span>grad)
</span></span></code></pre></div><p>torch支持控制流梯度计算，这意味着即使不同情况下计算b的方式可能不同，对b进行反向传播仍然可以得到当前计算方式下的对应梯度。</p>
<hr>
<h3 id="绘制函数与导函数图像不直接计算导数">绘制函数与导函数图像（不直接计算导数）<a href="#%e7%bb%98%e5%88%b6%e5%87%bd%e6%95%b0%e4%b8%8e%e5%af%bc%e5%87%bd%e6%95%b0%e5%9b%be%e5%83%8f%e4%b8%8d%e7%9b%b4%e6%8e%a5%e8%ae%a1%e7%ae%97%e5%af%bc%e6%95%b0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib_inline <span style="color:#f92672">import</span> backend_inline
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> d2l <span style="color:#f92672">import</span> torch <span style="color:#66d9ef">as</span> d2l
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">use_svg_display</span>():  <span style="color:#75715e">#@save</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;使用svg格式在Jupyter中显示绘图&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    backend_inline<span style="color:#f92672">.</span>set_matplotlib_formats(<span style="color:#e6db74">&#39;svg&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_figsize</span>(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">3.5</span>, <span style="color:#ae81ff">2.5</span>)):  <span style="color:#75715e">#@save</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;设置matplotlib的图表大小&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    use_svg_display()
</span></span><span style="display:flex;"><span>    d2l<span style="color:#f92672">.</span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;figure.figsize&#39;</span>] <span style="color:#f92672">=</span> figsize
</span></span><span style="display:flex;"><span><span style="color:#75715e">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_axes</span>(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;设置matplotlib的轴&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    axes<span style="color:#f92672">.</span>set_xlabel(xlabel)
</span></span><span style="display:flex;"><span>    axes<span style="color:#f92672">.</span>set_ylabel(ylabel)
</span></span><span style="display:flex;"><span>    axes<span style="color:#f92672">.</span>set_xscale(xscale)
</span></span><span style="display:flex;"><span>    axes<span style="color:#f92672">.</span>set_yscale(yscale)
</span></span><span style="display:flex;"><span>    axes<span style="color:#f92672">.</span>set_xlim(xlim)
</span></span><span style="display:flex;"><span>    axes<span style="color:#f92672">.</span>set_ylim(ylim)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> legend:
</span></span><span style="display:flex;"><span>        axes<span style="color:#f92672">.</span>legend(legend)
</span></span><span style="display:flex;"><span>    axes<span style="color:#f92672">.</span>grid()
</span></span><span style="display:flex;"><span><span style="color:#75715e">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot</span>(X, Y<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, xlabel<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, ylabel<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, legend<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, xlim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>         ylim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, xscale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>, yscale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>,
</span></span><span style="display:flex;"><span>         fmts<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;m--&#39;</span>, <span style="color:#e6db74">&#39;g-.&#39;</span>, <span style="color:#e6db74">&#39;r:&#39;</span>), figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">3.5</span>, <span style="color:#ae81ff">2.5</span>), axes<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;绘制数据点&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> legend <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        legend <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    set_figsize(figsize)
</span></span><span style="display:flex;"><span>    axes <span style="color:#f92672">=</span> axes <span style="color:#66d9ef">if</span> axes <span style="color:#66d9ef">else</span> d2l<span style="color:#f92672">.</span>plt<span style="color:#f92672">.</span>gca()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 如果X有一个轴，输出True</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">has_one_axis</span>(X):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> (hasattr(X, <span style="color:#e6db74">&#34;ndim&#34;</span>) <span style="color:#f92672">and</span> X<span style="color:#f92672">.</span>ndim <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">or</span> isinstance(X, list)
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">and</span> <span style="color:#f92672">not</span> hasattr(X[<span style="color:#ae81ff">0</span>], <span style="color:#e6db74">&#34;__len__&#34;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> has_one_axis(X):
</span></span><span style="display:flex;"><span>        X <span style="color:#f92672">=</span> [X]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> Y <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        X, Y <span style="color:#f92672">=</span> [[]] <span style="color:#f92672">*</span> len(X), X
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> has_one_axis(Y):
</span></span><span style="display:flex;"><span>        Y <span style="color:#f92672">=</span> [Y]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> len(X) <span style="color:#f92672">!=</span> len(Y):
</span></span><span style="display:flex;"><span>        X <span style="color:#f92672">=</span> X <span style="color:#f92672">*</span> len(Y)
</span></span><span style="display:flex;"><span>    axes<span style="color:#f92672">.</span>cla()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> x, y, fmt <span style="color:#f92672">in</span> zip(X, Y, fmts):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(x):
</span></span><span style="display:flex;"><span>            axes<span style="color:#f92672">.</span>plot(x, y, fmt)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            axes<span style="color:#f92672">.</span>plot(y, fmt)
</span></span><span style="display:flex;"><span>    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> d2l<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> math<span style="color:#f92672">.</span>pi, <span style="color:#ae81ff">0.05</span><span style="color:#f92672">*</span>math<span style="color:#f92672">.</span>pi)
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>requires_grad_(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> d2l<span style="color:#f92672">.</span>sin(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> f(x)<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot(x<span style="color:#f92672">.</span>detach(), [f(x)<span style="color:#f92672">.</span>detach(), x<span style="color:#f92672">.</span>grad],<span style="color:#e6db74">&#34;x&#34;</span>, <span style="color:#e6db74">&#34;f(x)&#34;</span>)
</span></span></code></pre></div><p>需要使用d2l库和jupyter lab。</p>
<hr>
<h2 id="概率">概率<a href="#%e6%a6%82%e7%8e%87" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<h3 id="多项分布multinomial-distribution">多项分布(multinomial distribution)<a href="#%e5%a4%9a%e9%a1%b9%e5%88%86%e5%b8%83multinomial-distribution" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>将概率分配给一些离散选项的分布称为多项分布。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.distributions <span style="color:#f92672">import</span> multinomial
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fair_probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones([<span style="color:#ae81ff">6</span>]) <span style="color:#f92672">/</span> <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(multinomial<span style="color:#f92672">.</span>Multinomial(<span style="color:#ae81ff">1</span>, fair_probs)<span style="color:#f92672">.</span>sample())
</span></span><span style="display:flex;"><span>print(multinomial<span style="color:#f92672">.</span>Multinomial(<span style="color:#ae81ff">2</span>, fair_probs)<span style="color:#f92672">.</span>sample())
</span></span><span style="display:flex;"><span>print(multinomial<span style="color:#f92672">.</span>Multinomial(<span style="color:#ae81ff">10</span>, fair_probs)<span style="color:#f92672">.</span>sample())
</span></span></code></pre></div><p><code>multinomial.Multinomial()</code>用来生成一个采样器，这个采样器每次采样会根据<code>fair_probs</code>也就是概率向量对应的概率来抽取一个位置索引，第一个参数时一次采样的抽取次数。最后得到的向量中对应位置的数字即这个位置被抽中的次数。</p>
<p>上述代码的某次生成结果为：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tensor([<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>])
</span></span><span style="display:flex;"><span>tensor([<span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>])
</span></span><span style="display:flex;"><span>tensor([<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">2.</span>, <span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">5.</span>])
</span></span></code></pre></div><hr>
<h3 id="联合概率">联合概率<a href="#%e8%81%94%e5%90%88%e6%a6%82%e7%8e%87" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>\(P(A=a,B=b)\)，A和B同时发生的概率。</p>
<h3 id="条件概率">条件概率<a href="#%e6%9d%a1%e4%bb%b6%e6%a6%82%e7%8e%87" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>\(0 \leq \frac{P(A=a, B=b)}{P(A=a)} \leq 1\) ，A发生的前提下A、B同时发生的概率。</p>
<h3 id="贝叶斯定理">贝叶斯定理<a href="#%e8%b4%9d%e5%8f%b6%e6%96%af%e5%ae%9a%e7%90%86" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
$$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}.$$<h3 id="边际化">边际化<a href="#%e8%be%b9%e9%99%85%e5%8c%96" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
$$P(B) = \sum_{A} P(A, B),$$
			</div>
			<div class="human posts"><a href="https://brainmade.org/" target="_blank" rel="external noreferrer noopener"><abbr title="I don't hate AIs; but I love humans!"><svg fill="#fff" width="128" height="40" viewBox="0 0 128 40" xmlns="http://www.w3.org/2000/svg">
      <path
         d="M26.306 39.391H11.665a1.28 1.28 0 0 1-1.28-1.28v-3.838H6.399a1.28 1.28 0 0 1-1.28-1.28v-5.336l-4.41-2.198a1.28 1.28 0 0 1-.493-1.855l4.904-7.357v-2.175C5.12 6.298 11.422 0 19.194 0s14.073 6.3 14.075 14.071c-.316 13.912-5.38 11.758-5.59 17.023l-.093 7.018a1.3 1.3 0 0 1-.375.905 1.27 1.27 0 0 1-.905.375zm-13.361-2.559h12.082l.143-7.27c-.132-3.329 5.858-4.122 5.54-15.368-.179-6.356-5.157-11.635-11.515-11.635S7.68 7.713 7.679 14.071v2.559c-.001.253-.075.5-.215.71l-4.315 6.471 3.822 1.91a1.28 1.28 0 0 1 .708 1.145v4.848h3.987a1.28 1.28 0 0 1 1.28 1.28z" />
      <path
         d="M20.186 29.111v-9.644c.059 0 .118.009.177.009 4.885-.006 8.525-4.506 7.511-9.284a1.19 1.19 0 0 0-.911-.911 7.67 7.67 0 0 0-9.049 5.67l-.033-.036a7.66 7.66 0 0 0-7.03-2.085 1.19 1.19 0 0 0-.911.91c-1.014 4.777 2.627 9.277 7.51 9.284.118 0 .246-.014.369-.02v6.106zm1.419-16.072a5.33 5.33 0 0 1 4.062-1.553 5.33 5.33 0 0 1-5.614 5.615 5.3 5.3 0 0 1 1.552-4.061zm-7.904 6.057a5.3 5.3 0 0 1-1.559-4.061 5.323 5.323 0 0 1 5.614 5.615 5.3 5.3 0 0 1-4.055-1.554m38.419-6.79q0 2.346-1.567 3.63-1.567 1.282-4.351 1.283h-7.669V0h7.016q2.807 0 4.242 1.1 1.446 1.087 1.446 3.226 0 1.467-.729 2.481-.718 1.002-2.197 1.357 1.86.244 2.828 1.32.979 1.063.979 2.823zm-4.112-7.491q0-1.161-.663-1.65-.652-.488-1.947-.488h-3.655v4.265h3.677q1.36 0 1.968-.525.62-.537.62-1.601m.892 7.209q0-2.42-3.089-2.42h-4.069v4.938h4.188q1.545 0 2.252-.624.718-.635.718-1.894m16.26 5.194-3.557-6.538h-3.764v6.538H54.63V0h7.658q2.741 0 4.231 1.332 1.49 1.32 1.49 3.8 0 1.808-.913 3.128-.914 1.308-2.47 1.723l4.144 7.235zm-.381-11.94q0-2.481-2.828-2.481h-4.112v5.083h4.199q1.349 0 2.045-.684.696-.685.696-1.919m16.785 11.94-1.36-4.399h-5.841l-1.359 4.399h-3.209L75.386 0h3.785l5.569 17.219zM77.278 2.651l-.066.269q-.109.44-.261 1.002c-.152.563-.725 2.436-1.871 6.184h4.405l-1.512-4.949-.468-1.662zm9.551 14.567V0h3.209v17.219zm15.53 0L95.681 3.959q.196 1.931.196 3.104v10.155h-2.849V0h3.666l6.777 13.37q-.196-1.846-.196-3.361V0h2.85v17.219zM52.63 39.375V28.331q.011-.351.115-3.015-.818 3.257-1.209 4.541l-2.925 9.518h-2.418l-2.925-9.518-1.232-4.541q.139 2.809.139 3.717v10.342h-3.017V22.313h4.548l2.902 9.543.253.92.553 2.289.725-2.736 2.983-10.015h4.526v17.063zm17.64 0-1.44-4.359h-6.184l-1.44 4.359h-3.397l5.919-17.063h4.007l5.896 17.063zm-4.537-14.434-.069.267q-.115.436-.277.993c-.162.557-.767 2.414-1.98 6.128h4.663l-1.601-4.905-.495-1.647zm24.577 5.776q0 2.64-.99 4.614-.979 1.961-2.787 3.003-1.796 1.042-4.122 1.042h-6.564V22.313h5.873q4.099 0 6.345 2.18 2.245 2.167 2.245 6.224m-3.42 0q0-2.748-1.359-4.19-1.359-1.453-3.881-1.453h-2.407v11.54h2.879q2.188 0 3.478-1.586t1.29-4.311m6 8.659V22.313h12.759v2.761h-9.362v4.287h8.66v2.761h-8.66v4.492h9.835v2.761zm15.75 0v-3.693h3.328v3.693zm12.445-12.149q2.082 0 3.662.781 1.58.78 2.422 2.235.833 1.454.833 3.393 0 2.98-1.845 4.676Q124.302 40 121.085 40q-3.208 0-5.005-1.688-1.798-1.688-1.798-4.695c0-3.007.606-3.57 1.817-4.694q1.817-1.696 4.986-1.696m0 2.702q-2.157 0-3.378.97-1.23.97-1.23 2.72 0 1.777 1.22 2.747 1.211.97 3.388.97 2.195 0 3.462-.988 1.258-.997 1.258-2.711 0-1.778-1.23-2.738-1.23-.97-3.491-.97m6.725-13.402-5.062 2.935v3.107h5.062v2.648h-13.331v-6.322q0-2.261 1.031-3.491 1.022-1.23 2.942-1.23 1.4 0 2.422.754 1.012.754 1.334 2.038l5.601-3.42zm-9.244.314q-1.92 0-1.921 2.334v3.393h3.936v-3.465q0-1.113-.53-1.688t-1.486-.575m7.249-10.915a6.5 6.5 0 0 0-.313-2.002q-.321-.969-.814-1.499h-1.845v3.088h-2.063V0h4.901q1.088 1.005 1.703 2.622a9.4 9.4 0 0 1 .615 3.375q0 3.088-1.798 4.748-1.808 1.661-5.119 1.661-3.292 0-5.043-1.67-1.76-1.67-1.76-4.803 0-4.452 3.473-5.664l.776 2.442q-1.012.395-1.533 1.238-.52.844-.52 1.984 0 1.867 1.192 2.837t3.416.97q2.261 0 3.501-.997 1.23-1.005 1.23-2.818z" />
   </svg></abbr></a></div>

<div class="related-posts thin">
	<h2>See Also</h2>
	<ul>
	
	<li><a href="/zh-cn/posts/venvminiconda/">工具不图鉴03：虚拟环境&amp;conda</a></li>
	
	</ul>
</div>

		</article>
		<aside id="toc">
			<div class="toc-title">Table of Contents</div>
			<nav id="TableOfContents">
  <ul>
    <li><a href="#多维数组基本知识">多维数组基本知识</a>
      <ul>
        <li><a href="#运算符">运算符</a></li>
        <li><a href="#广播机制">广播机制</a></li>
        <li><a href="#切片">切片</a></li>
        <li><a href="#转换为其他对象">转换为其他对象</a></li>
      </ul>
    </li>
    <li><a href="#pandas">pandas</a>
      <ul>
        <li><a href="#处理缺失值">处理缺失值</a></li>
        <li><a href="#pandas部分练习">pandas部分练习</a></li>
      </ul>
    </li>
    <li><a href="#线性代数">线性代数</a>
      <ul>
        <li><a href="#点积">点积</a></li>
        <li><a href="#向量积">向量积</a></li>
        <li><a href="#矩阵乘法">矩阵乘法</a></li>
        <li><a href="#范数">范数</a></li>
        <li><a href="#l2范数">L2范数</a></li>
        <li><a href="#l1范数">L1范数</a></li>
      </ul>
    </li>
    <li><a href="#自动求导autograd">自动求导（autograd）</a>
      <ul>
        <li><a href="#为什么xgrad似乎并不符合通常的程序设计原则">为什么x.grad似乎并不符合通常的程序设计原则？</a></li>
        <li><a href="#非标量变量的反向传播">非标量变量的反向传播</a></li>
        <li><a href="#分离计算">分离计算</a></li>
        <li><a href="#如何实现二阶导">如何实现二阶导?</a></li>
        <li><a href="#控制流">控制流</a></li>
        <li><a href="#绘制函数与导函数图像不直接计算导数">绘制函数与导函数图像（不直接计算导数）</a></li>
      </ul>
    </li>
    <li><a href="#概率">概率</a>
      <ul>
        <li><a href="#多项分布multinomial-distribution">多项分布(multinomial distribution)</a></li>
        <li><a href="#联合概率">联合概率</a></li>
        <li><a href="#条件概率">条件概率</a></li>
        <li><a href="#贝叶斯定理">贝叶斯定理</a></li>
        <li><a href="#边际化">边际化</a></li>
      </ul>
    </li>
  </ul>
</nav>
		</aside>
		<div class="post-nav thin">
			<a class="next-post" href="http://localhost:1313/zh-cn/posts/fabric%E8%87%AA%E5%AD%A6%E6%97%A5%E5%BF%9701%E5%86%9C%E5%A4%AB%E4%B9%90%E4%BA%8B%E9%87%8D%E7%BB%87%E7%89%88/">
				<span class="post-nav-label"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left">
      <line x1="19" y1="12" x2="5" y2="12"></line>
      <polyline points="12 19 5 12 12 5"></polyline>
   </svg>&nbsp;Newer</span><br><span>Fabric自学日志01：农夫乐事重织版</span>
			</a>
			<a class="prev-post" href="http://localhost:1313/zh-cn/posts/venvminiconda/">
				<span class="post-nav-label">Older&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right">
      <line x1="5" y1="12" x2="19" y2="12"></line>
      <polyline points="12 5 19 12 12 19"></polyline>
   </svg></span><br><span>工具不图鉴03：虚拟环境&amp;conda</span>
			</a>
		</div>
		<div id="comments" class="thin"><script src="https://giscus.app/client.js"
        data-repo="Moonhalf383/Hermit-v2-blog-yorozumoon"
        data-repo-id="R_kgDOQaFILg"
        data-category="Announcements"
        data-category-id="DIC_kwDOQaFILs4CyFGz"
        data-mapping="title"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="transparent_dark"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
</div>
	</main>
<footer id="site-footer" class="section-inner thin animated fadeIn faster">
<p>
	&copy; 2026 <a href="http://localhost:1313/">Yorozumoon</a>
	&#183; copyright by Moonhalf
	&#183; Made with <a href="https://gohugo.io/" target="_blank" rel="noopener" title="The world's fastest framework for building websites">Hugo</a> &amp; <a href="https://github.com/1bl4z3r/hermit-V2" target="_blank" rel="noopener" title="A fast, minimalist Hugo theme">Hermit-V2</a>
	</p></footer>
<script async src="http://localhost:1313/js/bundle.min.a2910447d5c22e84c4b04382d8c10c056b2b9d3e15c64d1fa04882359d61afd3.js" integrity="sha256-opEER9XCLoTEsEOC2MEMBWsrnT4Vxk0foEiCNZ1hr9M=" crossorigin="anonymous"></script><script async src="http://localhost:1313/js/link-share.min.24409a4f6e5537d70ffc55ec8f9192208d718678cb8638585342423020b37f39.js" integrity="sha256-JECaT25VN9cP/FXsj5GSII1xhnjLhjhYU0JCMCCzfzk=" crossorigin="anonymous"></script><script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
